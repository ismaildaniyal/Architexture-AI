# -*- coding: utf-8 -*-
"""model_implement.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sm8oNFoRlza4OSJQOb6uu7duBz2DKjcQ
"""

import numpy as np
import os
# Load the matrices
# Get the absolute path of the current file
base_dir = os.path.dirname(os.path.abspath(__file__))

# Construct paths to the .npy files
adj_path = os.path.join(base_dir, "adjacency_matrix.npy")
vec_path = os.path.join(base_dir, "room_matrix.npy")

# Load the .npy files
adj = np.load(adj_path, allow_pickle=True)
vec = np.load(vec_path, allow_pickle=True)
# print(adj)
# print(vec)
adj = np.array(adj, dtype=np.float32)
vec = np.array(vec, dtype=np.float32)
# print(adj)
# print(vec)
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import math
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Define the GCN Layer
class GraphTransformerLayer(nn.Module):
    def __init__(self, hidden_dim, num_heads=4, dropout=0.1):
        super(GraphTransformerLayer, self).__init__()
        self.attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout)
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.ReLU(),
            nn.Linear(hidden_dim * 4, hidden_dim)
        )

    def forward(self, X):
        # For MultiheadAttention, we treat X as (seq_len, batch, embed_dim).
        # Here we assume batch size = 1, so we unsqueeze and squeeze back.
        X_unsq = X.unsqueeze(1)            # Shape: (N, 1, hidden_dim)
        attn_out, _ = self.attn(X_unsq, X_unsq, X_unsq)
        X_norm = self.norm1(X_unsq + attn_out)
        mlp_out = self.mlp(X_norm)
        X_out = self.norm2(X_norm + mlp_out)
        return X_out.squeeze(1)            # Back to shape: (N, hidden_dim)

# Graph Transformer Model.
class GraphTransformer(nn.Module):
    def __init__(self, in_features, hidden_dim, out_features, num_layers=2, num_heads=4):
        super(GraphTransformer, self).__init__()
        # in_features is 33 (19 original features + 14 from A)
        self.embedding = nn.Linear(in_features, hidden_dim)  # Projects 33 -> hidden_dim
        self.layers = nn.ModuleList([
            GraphTransformerLayer(hidden_dim, num_heads) for _ in range(num_layers)
        ])
        self.output_layer = nn.Linear(hidden_dim, out_features)  # Projects hidden_dim -> out_features (33)

    def forward(self, X, A):
        # Pre-concatenate the features with A.
        # Here, we assume that A is already a row-wise representation (shape: (N,14))
        X_cat = torch.cat([X, A], dim=-1)  # (N, 19+14 = 33)
        X_emb = self.embedding(X_cat)       # (N, hidden_dim)
        for layer in self.layers:
            X_emb = layer(X_emb)            # (N, hidden_dim)
        return self.output_layer(X_emb)     # (N, out_features=33)

# MLP remains unchanged.
class MLP(nn.Module):
    def __init__(self, in_features, hidden_features, out_features):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.ln = nn.LayerNorm(hidden_features)

    def forward(self, X):
        out = F.relu(self.ln(self.fc1(X)))
        return self.fc2(out)

# Graph Transformer + MLP Model.
class GraphTransformerMLPModel(nn.Module):
    def __init__(self, feature_dim, hidden_dim, num_layers, num_heads, mlp_hidden_features):
        super(GraphTransformerMLPModel, self).__init__()
        # feature_dim is 33 (after concatenation of 19 features and 14 from A)
        self.graph_transformer = GraphTransformer(feature_dim, hidden_dim, feature_dim, num_layers, num_heads)
        self.mlp = MLP(feature_dim, mlp_hidden_features, 4)
        self.ln = nn.LayerNorm(feature_dim)

    def forward(self, X, A):
        # Pre-concatenate to get a 33-dim input.
        X_cat = torch.cat([X, A], dim=-1)  # (N, 33)
        # Transformer output (should also be 33-dim per node)
        Y = self.graph_transformer(X, A)
        # Residual connection: add the concatenated input with the normalized transformer output.
        S = X_cat + self.ln(Y)
        return self.mlp(S)

# Initialize the model.
# Note: feature_dim here is 33 (i.e. 19 + 14).
model = GraphTransformerMLPModel(
    feature_dim=33,          # 19 (node features) + 14 (from A)
    hidden_dim=64,
    num_layers=3,
    num_heads=4,
    mlp_hidden_features=512
)

# model.load_state_dict(torch.load('gcn_mlp_model_state3.pth'), strict=False)
# Get the directory of the current file
base_dir = os.path.dirname(os.path.abspath(__file__))

# Construct the absolute path to the .pth file
model_path = os.path.join(base_dir, "GraphTransformer.pth")

# Load the model state
model.load_state_dict(torch.load(model_path, weights_only=True), strict=False)

model.eval()
adj_matrix = torch.tensor(adj)  # Adjacency matrix (N, N)
feature_matrix = torch.tensor(vec)  # Feature matrix (N, D)
# print("after torch",adj_matrix)
# print("after torch",feature_matrix)
# Perform prediction with no gradient calculation
with torch.no_grad():
    predictions = model(feature_matrix, adj_matrix)  # Forward pass
    # print("Predicted output for the building:")

    predictions = np.array(predictions, dtype=np.float32)
    # print(predictions)


import matplotlib.pyplot as plt
import numpy as np

def plot_bounding_boxes_corners(bounding_boxes, labels=None):
    """
    Plot bounding boxes given in [b_x, b_y, a_x, a_y] format.
    """
    fig, ax = plt.subplots()

    for i, box in enumerate(bounding_boxes):
        b_x, b_y, a_x, a_y = box
        width = a_x - b_x
        height = a_y - b_y

        # Create a rectangle
        rect = plt.Rectangle((b_x, b_y), width, height, linewidth=2, edgecolor='r', facecolor='none')
        ax.add_patch(rect)

        # Add labels if provided
        if labels:
            plt.text(b_x + width / 2, b_y + height / 2, labels[i], ha='center', va='center', color='blue')

    # Configure plot
    plt.xlim(0, max(bounding_boxes[:, 2]) + 1)  # Extend the limit based on data
    plt.ylim(0, max(bounding_boxes[:, 3]) + 1)
    plt.gca().invert_yaxis()  # Invert y-axis for a top-left origin
    plt.xlabel("X-axis")
    plt.ylabel("Y-axis")
    plt.title("Bounding Boxes")
    plt.show()


bounding_boxes = predictions

# plot_bounding_boxes_corners(bounding_boxes)